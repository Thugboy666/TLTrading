# Models

Place external GGUF model files here (not committed). Configure llama.cpp server to use them and expose an OpenAI-compatible HTTP endpoint (see `scripts/run_llama_server.ps1`).
