Metadata-Version: 2.4
Name: thelighttrading
Version: 0.1.0
Summary: Multi-LLM trading prototype
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: fastapi
Requires-Dist: uvicorn[standard]
Requires-Dist: typer
Requires-Dist: pydantic
Requires-Dist: pydantic-settings
Requires-Dist: python-dotenv
Requires-Dist: requests
Requires-Dist: pynacl
Requires-Dist: PyYAML
Dynamic: license-file

# TheLightTrading

TheLightTrading is a Windows-first prototype for multi-LLM trading research. It ships with a node-based pipeline, per-node persistent memory, llama.cpp integration, and a minimal Three.js GUI. The repository is self-contained; LLM models are external GGUF files and not committed.

## Quick start (Windows PowerShell)

```powershell
# one-time setup
scripts\setup_windows.ps1

# bootstrap runtime (creates runtime/.env.example if missing)
scripts\reset_runtime.ps1
Copy-Item runtime/.env.example runtime/.env -ErrorAction SilentlyContinue

# manually download llama.cpp server.exe into runtime/bin/llama
# place chat GGUF model under runtime/models/chat (for example chat.gguf)
# optional: place embedding model under runtime/models/embed

# start LLM server in background (defaults to http://127.0.0.1:8081)
# model selection prefers LOCAL_CHAT_MODEL_DEFAULT, then LOCAL_CHAT_MODEL_QWEN, then LOCAL_CHAT_MODEL_MISTRAL
scripts\start_llm_bg.ps1

# start API in background (defaults to http://127.0.0.1:8080) and check status
scripts\start_api_bg.ps1
scripts\status_check.ps1
scripts\health_check.ps1 -CheckLlm

# stop API when finished
scripts\stop_api.ps1
# stop LLM server when finished
scripts\stop_llm.ps1

# run GUI (served from the same API port)
scripts\run_gui.ps1

# run pipeline via CLI
thelighttrading run-pipeline

# run smoke tests
scripts\smoke_test.ps1
```

Copy `runtime/.env.example` to `runtime/.env` and provide signing keys if you want signed ActionPackets. Without keys, the system generates HOLD packets marked UNSIGNED.

See `docs/windows_runtime.md` for background start/stop helpers and additional notes.

## Local LLM runtime layout

```
runtime/
  bin/llama/       # place llama.cpp server.exe here (download manually)
  models/chat/     # place chat GGUF models here
  models/embed/    # optional: embedding GGUF models
  logs/            # captured stdout/stderr for API and LLM servers
  state/           # PID/status files
```

The PowerShell scripts read `runtime/.env` when it exists and will not overwrite it. Configure model filenames with `LOCAL_CHAT_MODEL_DEFAULT`, `LOCAL_CHAT_MODEL_QWEN`, `LOCAL_CHAT_MODEL_MISTRAL`, and `LOCAL_EMBED_MODEL` and point `LOCAL_LLM_SERVER_URL` / `LLM_BASE_URL` at your llama.cpp server. Update `LLM_MODE=real` after launching the llama.cpp server with `scripts/start_llm_bg.ps1`.

To stop and start the stack:

```powershell
scripts\stop_llm.ps1
scripts\start_llm_bg.ps1
scripts\start_api.ps1
```

Models live under `runtime/models/chat` and `runtime/models/embed`. Point the env vars above at your GGUF filenames; relative paths are resolved from the repo root at runtime.

## Modes
- `LLM_MODE=mock` (default): deterministic mock outputs suitable for tests.
- `LLM_MODE=real`: sends OpenAI-compatible chat requests to a llama.cpp server at `LLM_BASE_URL`.

## Pipeline
1. NewsNode (news_llama) → summary
2. ParserNode (parser_qwen) → structured signals
3. BrainNode (brain_mistral) → strategy JSON
4. WatchdogNode (watchdog_phi) → risk gate
5. PacketNode → ActionPacket signing/validation with anti-replay

Runs are stored under `data/state/runs/<run_id>.json`. Node memories persist in `data/memory/thelighttrading.db`.

## GUI
Open `http://127.0.0.1:8080` (the API port) to view the node graph and run the pipeline. The GUI communicates with the FastAPI backend and displays last node outputs.

## Tests
Run `pytest` (defaults to mock LLM mode). CI enforces these tests.

## Safety
ActionPackets include policy hashes, nonces, sequences, expiries, and Ed25519 signatures when keys are supplied. Missing keys cause HOLD UNSIGNED packets for safety.
